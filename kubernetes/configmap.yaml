apiVersion: v1
data:
  config.txt: |
    # Directory where to store the VectorDB
    vectorstore=/vectorstore
    # Ollama model for text rendering
    embeddingmodel=nomic-embed-text
    # Ollama model for deep think
    mainmodel=deepseek-r1:7b
    # Deep of the web crawler
    crawldeep=2
    # Used vector results for deep think, be careful
    vectorresults=4
    # Listen address for web ui
    servername=0.0.0.0
    # Listen port for web ui
    serverport=9000
    # Web UI theme options: Base, Default, Glass, Monochrome, Soft
    theme=Soft
    # Performance tuning
    # Chunk size for document processing (1500-2500 recommended)
    chunk_size=2000
    # Chunk overlap for better context continuity (200-300 recommended)
    chunk_overlap=250
    # Maximum cache size for embeddings (affects memory usage)
    embedding_cache_size=500
    # Rate limiting between requests (seconds)
    crawl_delay=1
kind: ConfigMap
metadata:
  name: ollama-chatbot
---
apiVersion: v1
data:
  urls.txt: |
    https://k8sblog.eumel.de
    https://github.com/eumel8/cosignwebhook/blob/main/README.md
    https://api.github.com/repos/eumel8/cosignwebhook
    #https://github.com/eumel8/overlaytest/blob/main/README.md
    #http://gitrepo.ollama-chatbot/repo/
kind: ConfigMap
metadata:
  name: urls
---
apiVersion: v1
data:
  run.sh: |
    #!/bin/sh
    ollama pull nomic-embed-text
    ollama pull deepseek-r1:7b
    python3 web_crawler.py --urls ./urls/urls.txt --config ./chatbot-config/config.txt
    touch /vectorstore/crawl_ready
    sleep infinity
kind: ConfigMap
metadata:
  name: run-crawler
---
apiVersion: v1
data:
  run.sh: |
    #!/bin/sh
    while [ ! -e "/vectorstore/crawl_ready" ]; do
    echo "Crawler not ready, waiting..."
    sleep 5
    done
    echo "Crawler finished, started ui"
    python3 web_ui.py --config ./chatbot-config/config.txt
kind: ConfigMap
metadata:
  name: run-ui
